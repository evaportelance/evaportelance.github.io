---
layout: default
---


<section class="research">
	<div>
		<h2>Projects</h2>
		<p>
		<b>Objective functions for language learning</b> - Most language models currently learn by trying to maximize the log-likelihood of the next token. Intuitively, this does not seem to correspond with what children do when learning a language. How can we design models that learn more like humans? In this project, we compare how different objective functions during learning affect a model's performance on a set of cognitively informed tasks.
		</p>

		<p>
		<b>Neural grammar induction for mildly context sensitive languages</b> - Following the Compound Probabilistic Context-Free Grammar of <a href="https://www.aclweb.org/anthology/P19-1228/">Kim et al. 2019</a>, this project proposes to generalize neural grammar induction to <a href="https://arxiv.org/abs/1710.11301">Abstract Grammars</a> in order to learn more expressive grammars, such as Minimalist Grammars.
		</p>

	</div>
</section>

<section class="research">
	<div>
		<h2>Publications</h2>
		<p>
			<a href="https://cogsci.mindmodeling.org/2020/papers/0184/0184.pdf"><b>Portelance, E.</b>, J. Degen, M. C. Frank. (2020). Predicting Age of Acquisition in Early Word Learning Using Recurrent Neural Networks. <i>Proceedings of CogSci 2020.</i></a>
		</p>
		<p>
			<a href="./pdfs/LithuanianEllipsis_NELS2019.pdf"><b>Portelance, E.</b> (2020). Genuine Verb stranding VP-ellipsis in Lithuanian. <i>Proceedings of NELS 50.<i></a>
		</p>
		<p>
			<a href="https://arxiv.org/abs/1710.11350"><b>Portelance, E.</b>, A. Bruno, D. Harasim, L. Bergen, T. J. O'Donnell. (2019). Grammar Induction for Minimalist Grammars using Variational Bayesian Inference. <i>arXiv</i>:1710.11350</a>
		</p>
		<p>
			<a href="https://arxiv.org/abs/1710.11301">Harasim, D.,  A. Bruno, <b>E. Portelance</b>, M. Rohrmeier, T. J. O’Donnell. (2018). A generalised parsing framework for Abstract Grammars. <i>arXiv</i>:1710.11301.</a>
		</p>
		<p>
		<a href="http://post45.research.yale.edu/2016/05/how-cultural-capital-works-prizewinning-novels-bestsellers-and-the-time-of-reading/"><b>Portelance, E.</b> and A. Piper. (2016). How Cultural Capital Works: Prizewinners, Bestsellers, and the Time of Reading. <i>Post-45</i>.</a>
		</p>
	</div>
</section>

<section class="research">
	<div>
		<h2>Presentations and Posters</h2>
		<p>
			<br>
		<a href="./pdfs/Poster_BUCLD2019.pdf"><b>Portelance, E.</b>, G. Kachergis, M.C. Frank. (2019). <i>Comparing memory-based and neural network models of early syntactic development.</i> Poster presentation at the BUCLD, Boston, MA.</a>
		</p>
		<p>
			<b>Portelance, E.</b> (2019). <i>Verb stranding ellipsis in Lithuanian: verbal identity and head movement. </i>Presentation at the Syntax & Semantics circle, UC Berkeley.
		</p>
		<p>
			<a href="./pdfs/Poster_L2HM2018.pdf"><b>Portelance, E.</b>, A. Bruno, D. Harasim, L. Bergen, T. J. O’Donnell. (2018).<i> A Framework for Lexicalized Grammar Induction Using Variational Bayesian Inference. </i>Poster presentation at the Learning Language in Humans and Machines conference, Paris, France.</a>
		</p>
		<p>
			<b>Portelance, E.</b> (2018).<i> On the move: Free word order in Lithuanian. </i>Presentation at the Association for the Advancement of Baltic Studies Conference, Stanford, USA.
		</p>
		<p>
			<b>Portelance, E.</b>, A. Bruno, and T. J. O’Donnell. (2017).<i> Unsupervised induction of natural language dependency structures. </i>Poster presentation at the Montreal AI Symposium, Montreal, Canada.
		</p>
		<p>
			<a href="./pdfs/Poster_DH2017.pdf"><b>Portelance, E.</b> and A. Piper. (2017). Understanding Narrative: Computational approaches to detecting narrative frames. In proceedings of <i>Digital Humanities Conference 2017 </i>,  Montreal, Canada.</a>
		</p>
	</div>
</section>
